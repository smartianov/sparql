#!/usr/bin/env python3

# Это новый вариант, где в случае если схожий пользователь фильм не смотрел,
# его оценка не будет учитываться, и следующий по схожести браться не будет
# (то есть, по факту k будет не 4, а меньше для некоторых фильмов). Но при этом
# в алгоритме рекомендации появился некоторый нюанс.

import numpy as np
import pandas as pd
from SPARQLWrapper import SPARQLWrapper, JSON
import requests

class RecSys():
    '''Класс рекомендательной системы.
    Для выполнения тех или иных операций необходимо создать новый экземпляр
    класса, передав в конструктор пути до файлов с данными.
    Внутренние (скрытые) элементы класса начинаются с двух нижних подчеркиваний,
    это исключительно следование соглашению.'''
    def __init__(self, *, data, days, places, k=4, eval_depth=3):
        '''Конструктор принимает следующие именованные параметры:
        data - путь к файлу с оценками пользователей,
        days - путь к файлу с данными о днях просмотра
        places - путь к файлу с данными о местах просмотра
        Так же могут быть переданы дополнительные параметры:
        k - параметр k для алгоритма kNN
        eval_depth - глубина промежуточных вычислений
        (сколько знаков после запятой)'''

        # Параметр k для алгоритма kNN
        self.__k = k if type(k) == int and k >= 0 else 4

        # Количество знаков после запятой для временных вычислений
        # По умолчанию 3, как сказано в задании, но может быть любым целым
        # числом либо None (не округлять), что более точно и логично на мой
        # взгляд, так что такая возможность есть.
        self.__eval_depth = eval_depth if (
            type(eval_depth) == int and eval_depth >= 0
            or eval_depth == None
        ) else 3

        # Извлекаем данные и сохраняем в экземпляре класса

        # Извлеченный Pandas DataFrame сразу же был преобразован в массив
        # NumPy. Причина - лично мне так удобнее. В данном случае, обратиться
        # к оценке i-го фильма k-ым пользователем можно так:
        # self.__data[k][i]
        # С DataFrame для меня работа сложнее и непривычные, чем с обычным
        # двумерным массивом. Главное учитывать, что индексирование идет с нуля.
        # Также сразу вырезается первый столбец, ибо там никакой дельной
        # информации, и он будет мешать при обращении по индексам.
        self.__data = np.array(pd.read_csv(data))[:, 1:]
        
        # Аналогично переводим в NumPy массивы, единственное, что стоит помнить,
        # что это уже массивы массивов строк, а не чисел, в том числе, если
        # где-то значение -1, то это строка '-1', а не число, впрочем, это не
        # вызовет больших проблем. 
        self.__days = np.array(pd.read_csv(days), np.dtype(str))[:, 1:]
        self.__places = np.array(pd.read_csv(places), np.dtype(str))[:, 1:]

        # Вырезаем пробелы по краям
        self.__days = np.char.strip(self.__days)
        self.__places = np.char.strip(self.__places)

        # В эту переменную кэшируются уже посчитанные сходства,
        # так как они довольно часто нужны
        self.__tmp_sims = {}
    
    def __round(self, x):
        '''Округление временного значения x до заданной точности.'''
        return (
            x if self.__eval_depth == None else np.round(x, self.__eval_depth)
        )

    def __sim(self, u, v):
        '''Расчет сходства между двумя пользователями. Параметры u и v
        являются векторами оценок всех фильмов от данных пользователей,
        где оценка n-го фильма расположена на n-ой позиции (начиная с 0).'''

        # Для наиболее эффективной работы NumPy лучше для все операции
        # проводить ее встроенными средствами. Цикл for является нативным 
        # средством Python, и хотя если его правильно записать, не создавать
        # промежуточные списки и прочие встроенные структуры данных он должен
        # быть тоже весьма производительным. Однако в данном случае в нем нет
        # необходимости вовсе. Если внимательно посмотреть на формулу, то видно,
        # что в числителе стоит скалярное проихведение векторов, а в знаменателе
        # произведение норм в пространстве L2 (такую норму также называют
        # евклидовой). Для расчета этих вещей есть встроенные в NumPy средства.
        # 
        # Правда есть небольшой нюанс. Что в скалярном произведении, что в норме
        # итерации идут не по всем элементам, а только по тем, которые в обоих
        # векторах не равны -1. Поэтому сначала нужно немного пребразовать
        # исходные векторы, убрав у обоих координаты с тех позиций, на которых
        # хотя бы в одном из них координата равна -1.

        # Эта нужная маска - что-то вроде вектора, а если точнее булев NumPy
        # массив, в котором на "нужных" позициях True, а на тех, которые нужно
        # отбросить False. Получен он весьма очевидным образом, используя
        # синтаксический сахар NumPy (но и производительность тоже).
        mask = (u != -1) & (v != -1)
        
        # Заменяем исходные векторы на нужные
        u = u[mask]
        v = v[mask]

        # Функция расчета нормы, вынесена в отдельную короткую переменную для
        # красоты (благо присваивание по ссылкам и память на это практически
        # не расходуется). Эта функция может считать разные нормы, но по
        # умолчанию ровно та, какая нужна в данной задаче.
        n = np.linalg.norm

        # Просто калька матиматической формулы. Оператор @ в NumPy выполняет
        # матричное произведение, но в случае с векторами оно же является и
        # скалярным.

        return self.__round(u @ v / (n(u) * n(v)))

    def __grade(self, user, film, *, day_conds=None, place_conds=None):
        '''Расчет оценки фильма film для пользователя user.
        Оба параметра начинаются с нуля.
        Дополнительные параметры:
        day_conds - список/кортеж дней (в формате, как в исходных данных), 
        в которые учитывается оценка
        place_conds - список/кортеж мест (в формате, как в исходных данных), 
        в которых учитывается оценка'''

        # Вектор оценок заданного пользователя
        u = self.__data[user]

        # Индекс фильма, пересохранен для краткости, изначально film,
        # чтобы было понятно из названия параметра, что это.
        i = film
        
        # Если в кэше есть сходства для данного пользователя, то они
        # ивлекаются, иначе считаются и сохраняются.
        if user in self.__tmp_sims:
            sims = self.__tmp_sims[user]
        else:
            # Данный метод проходит каждую ось с индексом 1 (горизонтальную)
            # из сохраненных данных передает как массив в лямбда-функцию, а
            # она считает сходство и возвращается массив посчитанных сходств.
            # Если посмотрять на то, как хранятся данные, то можно увидеть,
            # что считаются сходства пользователя u со всеми остальными и в
            # выходном массива на i-ой позиции сходство пользователя u с i-ым,
            # в том числе там есть сходство и с самим собой, которое равно
            # единице. 
            sims = np.apply_along_axis(
                lambda v: self.__sim(u, v), 1,
                self.__data
            )
            self.__tmp_sims[user] = sims

        # Теперь необходимо взять k пользователей (по заданию 4) с ближайшими
        # сходствами и по ним уже считать оценку. При этом пользователи,
        # которые не оценили данный фильм должны быть проишнорированы. 

        # Сортируем массив сходств (не на месте, то есть по факту копию массива)
        # и сохраняем ключи исходного массива в отсортированном виде.
        users_nums = sims.argsort()

        # Так как сортировка по убывнию, нужно слайсировать последних k
        # пользователей. Но самый последний пользователь есть текущий, значит
        # нужно взять от предпоследнего. В итоге получили массив их номеров
        # (индексов) k ближайших пользователей.
        users_nums = users_nums[-2:-(self.__k + 2):-1]

        # Снова получаем некую маску, в этот раз она показывает, какие
        # пользователи смотрели данный фильм, а какие нет. Немного подробнее:
        # 1) транспанируем матрицу с исходными данными, теперь индексами
        # являются фильмы и в результате можно получить массив оценок от всех
        # пользователей для данного фильма
        # 2) получаем, собственно, массив
        # 3) так как в user_nums индексы всех пользователей, отростированных
        # по убыванию по близости с данным, то передав его как индекс мы
        # отсортируем полученных на предыдущем шаге массив оценок по близости
        # 4) записываем True для тех, кто смотрел фильм, для других False
        mask = np.transpose(self.__data)[i][users_nums] != -1

        # Если были переданы параметрами дни, при которых нужно учитывать
        # оценку (а при других нет).
        if day_conds != None:
            # Создаем матрицу, где все элементы False (т.к. в исходных данных
            # нет None и быть не может, ибо там только строки), матрица
            # соответствует формату маски
            day_mask = np.transpose(self.__days)[i][users_nums] == None
            
            # Добавляем к новой маске тех пользователей, которые смотрели фильм
            # в заданные дни
            for dc in day_conds:
                day_mask |= np.transpose(self.__days)[i][users_nums] == dc
            
            # Пересекаем данную маску с "главной"
            mask &= day_mask

        # Аналогично дням просмотра, только теперь с местами просмотра
        if place_conds != None:
            place_mask = np.transpose(self.__places)[i][users_nums] == None
            for pc in place_conds:
                place_mask |= np.transpose(self.__places)[i][users_nums] == pc
            mask &= place_mask

        # Применяем маску, чтобы остались только "нужные" рользователи, то есть,
        # смотревшие фильм, возможно в определенном месте или время
        users_nums = users_nums[mask]

        # Средняя оценка просмотренных фильмов данного пользователя
        ru = np.mean(u[u != -1])

        # Если никто фильм не смотрел еще, то просто ее и возвращаем
        if len(users_nums) == 0:
            return self.__round(ru)

        # Получаем векторы оценок для нужных пользователей
        users = self.__data[users_nums]
        
        # Уже знакомым синтаксисом считается числитель дроби в формуле оценки
        num = np.sum(np.apply_along_axis(
            lambda v: self.__sim(u, v) * (v[i] - np.mean(v[v != -1])), 1, users
        ))

        # Знаменатель
        den = np.sum(np.abs(sims[users_nums]))

        # Возможно лучше было бы посчитать одним циклом и числитель, и
        # знаменатель, в данном случае так можно. Но NumPy и так
        # производительный, так что я придерживаюсь функционального стиля.

        return self.__round(ru + num / den)
        

    def eval_grades(self, user, *, day_conds=None, place_conds=None):
        '''Расчет оценок для фильмов, которые пользователь не смотрел.
        В данном случае параметр user - номер пользователя, начиная с единицы,
        то есть как в исходных данных. Возвращается словарь, где ключами
        являются непросмотренные фильмы, а значениями предположительная оценка.
        Дополнительные параметры:
        day_conds - список/кортеж дней (в формате, как в исходных данных), 
        в которые учитывается оценка
        place_conds - список/кортеж мест (в формате, как в исходных данных), 
        в которых учитывается оценка'''
        
        # Индекс пользователя в массиве с данными
        u = user - 1

        # Оценки всех фильмов от данного пользователя, а так же массив
        # индексов фильмов (в том же порядке)
        user_data = self.__data[u]
        movies = np.arange(0, len(user_data))
        
        # Делаем лямда-функцию, возвращающую оценку фильма, который
        # пердан параметром, функцией NumPy, то есть она принимает не
        # конкретный фильм и возвращает оценку для него, а может принять NumPy
        # массив фильмов и вернуть NumPy массив оценок.
        np_grade = np.frompyfunc(
            lambda i: self.__grade(
                u, i, day_conds=day_conds, place_conds=place_conds
            ), 1, 1
        )
        
        # Получаеи фильмы, которые пользователь не оценил и
        # считаем оценки для них.
        movies = movies[user_data == -1]
        grades = np_grade(movies)

        # К индексу каждого фильма добавляем единицу, чтобы получить его номер
        # из исходных данных.
        movies += 1

        # Объединяем массивы номеров фильмов и их оценок в одну матрицу 2xN
        # и преобразуем ее к обычному словарю Python, больше функционал и
        # производительность NumPy не понадобятся.
        result = dict(np.column_stack((movies, grades)))

        return result

    def flush_sims(self):
        '''Очистка кэша сходств.'''
        self.__tmp_sims = {}

    def change_k(self, k):
        '''Изменение коэффициента k.'''
        if type(k) == int and k >= 0:
            self.__k = k

    def change_eval_depth(self, depth):
        '''Изменение точности временных вычислений.'''
        if type(depth) == int and depth >= 0 or depth == None:
            self.__eval_depth = depth
    
    def movie_to_weekend(self, user):
        '''Рекомедация фильма пользователю для просмотра дома в выходной.'''
        
        # Так как в принципе у нас уже есть метод для рекомедаций, его и
        # применим. Нужно выбрать мильм с наивысшей оценкой из тех, что
        # пользователь еще не видел. Единственное отличие, что нужно учитывать
        # в расчетах не те оценки, а только те, которые были даны лишь теми
        # пользователями, которые смотрели данный фильм дома в выходные,
        # ибо оценки могут отличаться, например может быть повышена за атмосферу
        # кинотеатра или понижены за то, что был неудачный будний. Метод для
        # оценивания всех непросмотренных фильмов может принимать данные
        # параметры, так что просто их передаем.

        # Но данный метод заточен под k=4, в этой же реализации фактически k
        # может быть и 2, и 1, и вообще ноль. При единице, например, оценка
        # будет больше 5 (хоть и не сильно). Можно, конечно, использовать и
        # другой метод, вводить веса, но все равно нет гарантии, что этот самый
        # вес не поднимет ее выше 5 или не опустит ниже 1. Так что, это
        # несильное выхождение (до 0.5) за рамки [0,5] будем считать нормой.

        grades = self.eval_grades(
            user, place_conds=('h'), day_conds=('Sat', 'Sun')
        )

        # Выбираем фильм с максимальной оценкой и возвращаем как словарь (по
        # аналогии прошлому методу)
        return dict([max(grades.items(), key=lambda movie: movie[1])])

# Непосредственное использование

# Номер пользователя из базы
USER = 18

# Класс рекомендательной системы
rs = RecSys(
    data='data.csv',
    days='context_day.csv',
    places='context_place.csv'
)

# Получаем индекс рекомендованного фильма
movie_ind = tuple(rs.movie_to_weekend(USER).keys())[0] - 1

# Считывем название нужного фильма и убираем возможные лишние пробелы
movie = pd.read_csv("movie_names.csv", header=None, delimiter=',')[1][movie_ind]
movie = movie.strip()

# Через API ищем данный фильм на WikiData
res = requests.get('https://www.wikidata.org/w/api.php', params={
    'action' : 'wbsearchentities',
    'format' : 'json',
    'language' : 'en',
    'search': movie
})

# Получаем id данного фильма на WikiData
movie_id = res.json()['search'][0]['id']

# Запрос из предыдущего пункта задания для данного фильма
query = '''

SELECT DISTINCT ?filmLabel ?nominationLabel ?awardLabel
WHERE {
  {
    SELECT (MIN(YEAR(?publicationDate)) AS ?year)
    WHERE {
      wd:''' + movie_id + ''' wdt:P577 ?publicationDate.
    }
  }
  
  { ?nomination wdt:P31 wd:Q19020. }
  UNION
  { ?nomination wdt:P31 wd:Q28444913. }
  
  ?nomination wdt:P31 ?award.
  
  ?nominationStatement ps:P166 ?nomination;
                  pq:P585 ?nominationDate.

  ?film wdt:P31 wd:Q11424;
        p:P166 ?nominationStatement.
  
  FILTER((YEAR(?nominationDate)) = ?year && (?award = wd:Q19020 || ?award = wd:Q28444913))
  
  SERVICE wikibase:label {
    bd:serviceParam wikibase:language "en".
  }
 }

'''

# Подключаемся к серверу WikiData, он работает, видимо, не для всех user-agent,
# так что я указал тот, который рекомендуют на официальном форуме
sparql = SPARQLWrapper(
    'https://query.wikidata.org/sparql',
    agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        'AppleWebKit/537.36 (KHTML, like Gecko)'
        'Chrome/87.0.4280.88'
        'Safari/537.36'
)

# Задаем к подключению наш запрос и формат получемых данных
sparql.setQuery(query)
sparql.setReturnFormat(JSON)

# Выполняем запрос и извлекаем данные
answer = sparql.query().convert()

# Изучив немного структуру полученного ответа, находим в нем нужные нам данные
# и сохраняем в переменную result
result = pd.json_normalize(answer['results']['bindings'])
result = result[['filmLabel.value', 'nominationLabel.value', 'awardLabel.value']]

print(result.head())